{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.3.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\5483203\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\5483203\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.0/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 10.4 MB/s  0:00:01\n",
      "Downloading numpy-2.3.3-cp313-cp313-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.4/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.7/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 9.4 MB/s  0:00:01\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------------------------------------- 0/4 [pytz]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   ---------- ----------------------------- 1/4 [tzdata]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   -------------------- ------------------- 2/4 [numpy]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ------------------------------ --------- 3/4 [pandas]\n",
      "   ---------------------------------------- 4/4 [pandas]\n",
      "\n",
      "Successfully installed numpy-2.3.3 pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hODfjjsJK8uR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separarTreinoTeste(dfA):\n",
    "    dfB = dfA\n",
    "    dfC = pd.DataFrame()\n",
    "    for i in range (len(dfA)):\n",
    "        vetor = dfA.iloc[i, 0]\n",
    "        if (rd.random() > 0.8): # vai p teste\n",
    "            dfB = pd.concat([dfB, pd.DataFrame(data=[[vetor]], columns=['noticias'])], ignore_index=True)\n",
    "        else:\n",
    "            dfC = pd.concat([dfC, pd.DataFrame(data=[[vetor]], columns=['noticias'])], ignore_index=True)\n",
    "\n",
    "    return dfB, dfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepararDfTeste (dfA, dfB):\n",
    "    # embaralha os dfs\n",
    "    dfEmbaralhado = pd.concat([dfA, dfB], ignore_index=True)\n",
    "    indices = list(dfEmbaralhado.index)\n",
    "    rd.shuffle(indices)\n",
    "    \n",
    "    tags = []\n",
    "    noticias = []\n",
    "\n",
    "    # cria uma coluna pra tag\n",
    "    for i in indices:\n",
    "        news = dfEmbaralhado.iloc[i, 0]\n",
    "        tags.append(news[0])\n",
    "        noticias.append(news[1:])\n",
    "\n",
    "    return pd.DataFrame({'tag': tags, 'noticia': noticias})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepararDfsTreino (dfF, dfR):\n",
    "    # Falsas\n",
    "    palavrasFalsas = []\n",
    "    for i in range(len(dfF)):\n",
    "        noticia = dfF.iloc[i,0][1:]\n",
    "        palavrasFalsas.extend(noticia)\n",
    "    dfF = pd.DataFrame({'palavras': palavrasFalsas})\n",
    "    dfF = dfF.groupby('palavras').size().reset_index(name='aparicoesFalsas')\n",
    "\n",
    "    palavrasFalsas = []\n",
    "    for i in range(len(dfR)):\n",
    "        noticia = dfR.iloc[i,0][1:]\n",
    "        palavrasFalsas.extend(noticia)\n",
    "    dfR = pd.DataFrame({'palavras': palavrasFalsas})\n",
    "    dfR = dfR.groupby('palavras').size().reset_index(name='aparicoesReais')\n",
    "    palavras = []\n",
    "\n",
    "    return dfF, dfR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "P4VNs4t3K8uU"
   },
   "outputs": [],
   "source": [
    "def evaluate(dfReal, dfFalso, noticiasTeste, limiar, coeficiente):\n",
    "    # Junta os dataframes e une as palavras\n",
    "    dfJunto = dfReal.merge(dfFalso, how='outer')\n",
    "\n",
    "    # Conta quantas palavras tem em uma e não tem na outra\n",
    "    nReal = dfJunto.loc[dfJunto['aparicoesReais'].isna(), 'palavras'].count()\n",
    "    nFalsa = dfJunto.loc[dfJunto['aparicoesFalsas'].isna(), 'palavras'].count()\n",
    "\n",
    "    # Preenche as palavras que não aparecem com 1\n",
    "    dfJunto.fillna(1, inplace=True)\n",
    "\n",
    "    # Soma as palavras com 0 aparições com 1 para evitar multiplicação por 0\n",
    "    dfJunto.loc[dfJunto['aparicoesReais'] > 1, 'aparicoesReais'] += 1\n",
    "    dfJunto.loc[dfJunto['aparicoesFalsas'] > 1, 'aparicoesFalsas'] += 1\n",
    "\n",
    "    # Remove linhas onde a coluna 'palavras' é vazia ou só espaço\n",
    "    dfJunto = dfJunto[dfJunto['palavras'].str.strip() != '']\n",
    "\n",
    "    # Mantém só palavras que têm pelo menos uma letra (a–z ou A–Z ou acentuadas)\n",
    "    dfJunto = dfJunto[dfJunto['palavras'].str.match(r'^[A-Za-zÀ-ÖØ-öø-ÿ]+$', na=False)]\n",
    "\n",
    "    # Soma o total de aparições\n",
    "    totalReal = dfJunto['aparicoesReais'].sum()\n",
    "    totalFalso = dfJunto['aparicoesFalsas'].sum()\n",
    "\n",
    "    # Balanceia a ordem de grandeza das aparições\n",
    "    dfJunto['aparicoesFalsas'] *= totalReal / totalFalso\n",
    "    \n",
    "    # Faz o coeficiente de generalized jaccard\n",
    "    dfJunto['gj'] = np.minimum(dfJunto['aparicoesReais'], dfJunto['aparicoesFalsas']) / np.maximum(dfJunto['aparicoesReais'], dfJunto['aparicoesFalsas'])\n",
    "\n",
    "    # Faz o coeficiente de bray curtis\n",
    "    dfJunto['bc'] = abs(dfJunto['aparicoesReais'] - dfJunto['aparicoesFalsas']) / (dfJunto['aparicoesReais'] + dfJunto['aparicoesFalsas'])\n",
    "\n",
    "    # Limpa palavras com coeficiente menor que o limiar (palavras com aparições equivalentes em ambos casos)\n",
    "    if (coeficiente == 'bc'):\n",
    "        dfJunto = dfJunto.loc[dfJunto['bc'] > limiar]\n",
    "    elif (coeficiente == 'gj'):\n",
    "        dfJunto = dfJunto.loc[dfJunto['gj'] > limiar]\n",
    "\n",
    "    # Pega o total de aparições após NLP\n",
    "    tFalsas = dfJunto['aparicoesFalsas'].sum()\n",
    "    tReais = dfJunto['aparicoesReais'].sum()\n",
    "\n",
    "    # Define um df de teste\n",
    "    dfJuntoTeste = dfJunto\n",
    "\n",
    "    # Faz os calculos das probabilidades\n",
    "    dfJuntoTeste['aparicoesFalsas'] = dfJunto['aparicoesFalsas'] / (nFalsa + tFalsas)\n",
    "    dfJuntoTeste['aparicoesReais'] = dfJunto['aparicoesReais'] / (nReal + tReais)\n",
    "\n",
    "    # Pega o log na base 10\n",
    "    dfJuntoTeste['aparicoesFalsas'] = np.log10(dfJuntoTeste['aparicoesFalsas'])\n",
    "    dfJuntoTeste['aparicoesReais'] = np.log10(dfJuntoTeste['aparicoesReais'])\n",
    "\n",
    "    # NOTICIA:\n",
    "    \n",
    "    indices = ['fF', 'fR', 'rF', 'rR']\n",
    "    dfDados = pd.DataFrame(0, index=indices, columns=['num'])\n",
    "\n",
    "    for i in range (len(noticiasTeste)):\n",
    "        noticia = noticiasTeste.iloc[i, 1]\n",
    "        tag = noticiasTeste.iloc[i, 0]\n",
    "\n",
    "        # Cria um df com a notícia limpa\n",
    "        dfNoticia = pd.DataFrame(data = noticia, columns=['palavra'])\n",
    "\n",
    "        # Junta o df da noticia com o das probabilidades\n",
    "        dfNoticia = dfNoticia.merge(dfJuntoTeste, how='inner', left_on='palavra', right_on='palavras')\n",
    "\n",
    "        # Faz a soma das probabilidades\n",
    "        iFalsa = dfNoticia['aparicoesFalsas'].sum()\n",
    "        iReal = dfNoticia['aparicoesReais'].sum()\n",
    "\n",
    "        # Confere o resultado\n",
    "        if (iFalsa > iReal): # Falsa\n",
    "            resultado = 'F'\n",
    "        elif (iReal == iFalsa): # Empate\n",
    "            if (tag == 'f'):\n",
    "                resultado = 'R'\n",
    "            elif (tag == 'r'):\n",
    "                resultado = 'F'\n",
    "        else:\n",
    "            resultado = 'R'\n",
    "            \n",
    "        chave = tag + resultado\n",
    "        try:\n",
    "            dfDados.loc[chave, 'num'] += 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return dfDados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://distancia.readthedocs.io/en/latest/GeneralizedJaccard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m     limiar = i/\u001b[32m10\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(limiar)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     matrizDeConfusao = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfRealTreino\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfFalsoTreino\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdfTestes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimiar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     matrizesRounds.append(matrizDeConfusao)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# guarda a matriz de confusão\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dfReal, dfFalso, noticiasTeste, limiar, coeficiente)\u001b[39m\n\u001b[32m     66\u001b[39m dfNoticia = pd.DataFrame(data = noticia, columns=[\u001b[33m'\u001b[39m\u001b[33mpalavra\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Junta o df da noticia com o das probabilidades\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m dfNoticia = \u001b[43mdfNoticia\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdfJuntoTeste\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minner\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpalavra\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpalavras\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Faz a soma das probabilidades\u001b[39;00m\n\u001b[32m     72\u001b[39m iFalsa = dfNoticia[\u001b[33m'\u001b[39m\u001b[33maparicoesFalsas\u001b[39m\u001b[33m'\u001b[39m].sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\frame.py:10839\u001b[39m, in \u001b[36mDataFrame.merge\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10820\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m  10821\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents=\u001b[32m2\u001b[39m)\n\u001b[32m  10822\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmerge\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m  10835\u001b[39m     validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m ) -> DataFrame:\n\u001b[32m  10837\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10840\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10843\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10848\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10849\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10853\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     op = _MergeOperation(\n\u001b[32m    171\u001b[39m         left_df,\n\u001b[32m    172\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m         validate=validate,\n\u001b[32m    183\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[39m, in \u001b[36m_MergeOperation.get_result\u001b[39m\u001b[34m(self, copy)\u001b[39m\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right = \u001b[38;5;28mself\u001b[39m._indicator_pre_merge(\u001b[38;5;28mself\u001b[39m.left, \u001b[38;5;28mself\u001b[39m.right)\n\u001b[32m    886\u001b[39m join_index, left_indexer, right_indexer = \u001b[38;5;28mself\u001b[39m._get_join_info()\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[38;5;28mself\u001b[39m._merge_type)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.indicator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:879\u001b[39m, in \u001b[36m_MergeOperation._reindex_and_concat\u001b[39m\u001b[34m(self, join_index, left_indexer, right_indexer, copy)\u001b[39m\n\u001b[32m    877\u001b[39m left.columns = llabels\n\u001b[32m    878\u001b[39m right.columns = rlabels\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m result = \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[39m, in \u001b[36m_Concatenator.get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m             indexers[ax] = obj_labels.get_indexer(new_labels)\n\u001b[32m    682\u001b[39m     mgrs_indexers.append((obj._mgr, indexers))\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m new_data = \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[32m    688\u001b[39m     new_data._consolidate_inplace()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[39m, in \u001b[36mconcatenate_managers\u001b[39m\u001b[34m(mgrs_indexers, axes, concat_axis, copy)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m concat_axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     mgrs = \u001b[43m_maybe_reindex_columns_na_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_copy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[32m0\u001b[39m].concat_horizontal(mgrs, axes)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].nblocks > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[39m, in \u001b[36m_maybe_reindex_columns_na_proxy\u001b[39m\u001b[34m(axes, mgrs_indexers, needs_copy)\u001b[39m\n\u001b[32m    220\u001b[39m         mgr = mgr.reindex_indexer(\n\u001b[32m    221\u001b[39m             axes[i],\n\u001b[32m    222\u001b[39m             indexers[i],\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             use_na_proxy=\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[32m    228\u001b[39m         )\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m         mgr = \u001b[43mmgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m     new_mgrs.append(mgr)\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    601\u001b[39m         res._blklocs = \u001b[38;5;28mself\u001b[39m._blklocs.copy()\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[43mres\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[39m, in \u001b[36mBlockManager._consolidate_inplace\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1783\u001b[39m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[32m   1785\u001b[39m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[32m   1786\u001b[39m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[32m   1787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_consolidated():\n\u001b[32m-> \u001b[39m\u001b[32m1788\u001b[39m         \u001b[38;5;28mself\u001b[39m.blocks = \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m         \u001b[38;5;28mself\u001b[39m._is_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28mself\u001b[39m._known_consolidated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[39m, in \u001b[36m_consolidate\u001b[39m\u001b[34m(blocks)\u001b[39m\n\u001b[32m   2267\u001b[39m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   2268\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[32m-> \u001b[39m\u001b[32m2269\u001b[39m     merged_blocks, _ = \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2270\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[32m   2271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2272\u001b[39m     new_blocks = extend_blocks(merged_blocks, new_blocks)\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\5483203\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[39m, in \u001b[36m_merge_blocks\u001b[39m\u001b[34m(blocks, dtype, can_consolidate)\u001b[39m\n\u001b[32m   2301\u001b[39m     new_values = new_values[argsort]\n\u001b[32m   2302\u001b[39m     new_mgr_locs = new_mgr_locs[argsort]\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m     bp = \u001b[43mBlockPlacement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement=bp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2307\u001b[39m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#   BRAY CURTIS\n",
    "dfFalsoOriginal = pd.read_json('../data/train/fakeTrain.json', orient='records', lines=True)\n",
    "dfRealOriginal = pd.read_json('../data/train/realTrain.json', orient='records', lines=True)\n",
    "\n",
    "matrizesDeConfusao = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    dfFalso = dfFalsoOriginal.copy()\n",
    "    dfReal = dfRealOriginal.copy()\n",
    "\n",
    "    # separa em treino e teste\n",
    "    [dfFalsoTeste, dfFalsoTreino] = separarTreinoTeste(dfFalso)\n",
    "    [dfRealTeste, dfRealTreino] = separarTreinoTeste(dfReal)\n",
    "\n",
    "    # mistura os dados de teste\n",
    "    dfTestes = prepararDfTeste(dfFalsoTeste, dfRealTeste)\n",
    "\n",
    "    # prepara os dados de treino (junta as palavras e conta as aparições)\n",
    "    [dfFalsoTreino, dfRealTreino] = prepararDfsTreino(dfFalsoTreino, dfRealTreino)\n",
    "\n",
    "    # avalia o modelo\n",
    "    matrizesRounds = []\n",
    "    for i in range(1, 10): \n",
    "        limiar = i/10\n",
    "        print(limiar)\n",
    "        matrizDeConfusao = evaluate(dfRealTreino, dfFalsoTreino, dfTestes, limiar, 'bc')\n",
    "        matrizesRounds.append(matrizDeConfusao)\n",
    "    \n",
    "    # guarda a matriz de confusão\n",
    "    matrizesDeConfusao.append(matrizesRounds)\n",
    "\n",
    "mediasAcuracidadesBC = []\n",
    "mediasPrecisoesReaisBC = []\n",
    "mediasPrecisoesFalsasBC = []\n",
    "mediasSensibilidadeReaisBC = []\n",
    "mediasSensibilidadeFalsasBC = []\n",
    "mediasf1ScoreReaisBC = []\n",
    "mediasf1ScoreFalsasBC = []\n",
    "\n",
    "for i in range(0, 9):\n",
    "    acuracidades = []\n",
    "    precisoesReais = []\n",
    "    precisoesFalsas = []\n",
    "    sensibilidadeReais = []\n",
    "    sensibilidadeFalsas = []\n",
    "    f1Reais = []\n",
    "    f1Falsas = []\n",
    "\n",
    "    for j in range(1, len(matrizesDeConfusao)):\n",
    "        acuracidade = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            sum(matrizesDeConfusao[j][i]['num'])\n",
    "        ), 4)\n",
    "        acuracidades.append(acuracidade)\n",
    "\n",
    "        precReal = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['fR', 'num'])\n",
    "        ), 4)\n",
    "        precisoesReais.append(precReal)\n",
    "\n",
    "        precFalsa = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num'] + matrizesDeConfusao[j][i].loc['rF', 'num'])\n",
    "        ), 4)\n",
    "        precisoesFalsas.append(precFalsa)\n",
    "\n",
    "        sensReal = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['rF', 'num'])\n",
    "        ), 4)\n",
    "        sensibilidadeReais.append(sensReal)\n",
    "\n",
    "        sensFalsa = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num'] + matrizesDeConfusao[j][i].loc['fR', 'num'])\n",
    "        ), 4)\n",
    "        sensibilidadeFalsas.append(sensFalsa)\n",
    "\n",
    "        f1Real = round(2 * ((precReal * sensReal) / (precReal + sensReal)), 4)\n",
    "        f1Reais.append(f1Real)\n",
    "\n",
    "        f1Falsa = round(2 * ((precFalsa * sensFalsa) / (precFalsa + sensFalsa)), 4)\n",
    "        f1Falsas.append(f1Falsa)\n",
    "\n",
    "    mediaAcuracidades = round(sum(acuracidades) / len(acuracidades), 4)\n",
    "    mediasAcuracidadesBC.append(mediaAcuracidades)\n",
    "\n",
    "    mediaPrecisaoReais = round(sum(precisoesReais) / len(precisoesReais), 4)\n",
    "    mediasPrecisoesReaisBC.append(mediaPrecisaoReais)\n",
    "\n",
    "    mediaPrecisaoFalsas = round(sum(precisoesFalsas) / len(precisoesFalsas), 4)\n",
    "    mediasPrecisoesFalsasBC.append(mediaPrecisaoFalsas)\n",
    "\n",
    "    mediaSensibilidadeReais = round(sum(sensibilidadeReais) / len(sensibilidadeReais), 4)\n",
    "    mediasSensibilidadeReaisBC.append(mediaSensibilidadeReais)\n",
    "\n",
    "    mediaSensibilidadeFalsas = round(sum(sensibilidadeFalsas) / len(sensibilidadeFalsas), 4)\n",
    "    mediasSensibilidadeFalsasBC.append(mediaSensibilidadeFalsas)\n",
    "\n",
    "    mediaF1Reais = round(sum(f1Reais) / len(f1Reais), 4)\n",
    "    mediasf1ScoreReaisBC.append(mediaF1Reais)\n",
    "\n",
    "    mediaF1Falsas = round(sum(f1Falsas) / len(f1Falsas), 4)\n",
    "    mediasf1ScoreFalsasBC.append(mediaF1Falsas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   GENERALIZED JACCARD\n",
    "dfFalsoOriginal = pd.read_json('../data/train/fakeTrain.json', orient='records', lines=True)\n",
    "dfRealOriginal = pd.read_json('../data/train/realTrain.json', orient='records', lines=True)\n",
    "\n",
    "matrizesDeConfusao = [[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    dfFalso = dfFalsoOriginal.copy()\n",
    "    dfReal = dfRealOriginal.copy()\n",
    "\n",
    "    # separa em treino e teste\n",
    "    [dfFalsoTeste, dfFalsoTreino] = separarTreinoTeste(dfFalso)\n",
    "    [dfRealTeste, dfRealTreino] = separarTreinoTeste(dfReal)\n",
    "\n",
    "    # mistura os dados de teste\n",
    "    dfTestes = prepararDfTeste(dfFalsoTeste, dfRealTeste)\n",
    "\n",
    "    # prepara os dados de treino (junta as palavras e conta as aparições)\n",
    "    [dfFalsoTreino, dfRealTreino] = prepararDfsTreino(dfFalsoTreino, dfRealTreino)\n",
    "\n",
    "    # avalia o modelo\n",
    "    matrizesRounds = []\n",
    "    for i in range(1, 10): \n",
    "        limiar = i/10\n",
    "        print(limiar)\n",
    "        matrizDeConfusao = evaluate(dfRealTreino, dfFalsoTreino, dfTestes, limiar, 'gj')\n",
    "        matrizesRounds.append(matrizDeConfusao)\n",
    "    \n",
    "    # guarda a matriz de confusão\n",
    "    matrizesDeConfusao.append(matrizesRounds)\n",
    "\n",
    "mediasAcuracidadesGJ = []\n",
    "mediasPrecisoesReaisGJ = []\n",
    "mediasPrecisoesFalsasGJ = []\n",
    "mediasSensibilidadeReaisGJ = []\n",
    "mediasSensibilidadeFalsasGJ = []\n",
    "mediasf1ScoreReaisGJ = []\n",
    "mediasf1ScoreFalsasGJ = []\n",
    "\n",
    "for i in range(0, 9):\n",
    "    acuracidades = []\n",
    "    precisoesReais = []\n",
    "    precisoesFalsas = []\n",
    "    sensibilidadeReais = []\n",
    "    sensibilidadeFalsas = []\n",
    "    f1Reais = []\n",
    "    f1Falsas = []\n",
    "\n",
    "    for j in range(1, len(matrizesDeConfusao)):\n",
    "        acuracidade = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            sum(matrizesDeConfusao[j][i]['num'])\n",
    "        ), 4)\n",
    "        acuracidades.append(acuracidade)\n",
    "\n",
    "        precReal = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['fR', 'num'])\n",
    "        ), 4)\n",
    "        precisoesReais.append(precReal)\n",
    "\n",
    "        precFalsa = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num'] + matrizesDeConfusao[j][i].loc['rF', 'num'])\n",
    "        ), 4)\n",
    "        precisoesFalsas.append(precFalsa)\n",
    "\n",
    "        sensReal = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['rR', 'num'] + matrizesDeConfusao[j][i].loc['rF', 'num'])\n",
    "        ), 4)\n",
    "        sensibilidadeReais.append(sensReal)\n",
    "\n",
    "        sensFalsa = round(float(\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num']) /\n",
    "            (matrizesDeConfusao[j][i].loc['fF', 'num'] + matrizesDeConfusao[j][i].loc['fR', 'num'])\n",
    "        ), 4)\n",
    "        sensibilidadeFalsas.append(sensFalsa)\n",
    "\n",
    "        f1Real = round(2 * ((precReal * sensReal) / (precReal + sensReal)), 4)\n",
    "        f1Reais.append(f1Real)\n",
    "\n",
    "        f1Falsa = round(2 * ((precFalsa * sensFalsa) / (precFalsa + sensFalsa)), 4)\n",
    "        f1Falsas.append(f1Falsa)\n",
    "\n",
    "    mediaAcuracidades = round(sum(acuracidades) / len(acuracidades), 4)\n",
    "    mediasAcuracidadesGJ.append(mediaAcuracidades)\n",
    "\n",
    "    mediaPrecisaoReais = round(sum(precisoesReais) / len(precisoesReais), 4)\n",
    "    mediasPrecisoesReaisGJ.append(mediaPrecisaoReais)\n",
    "\n",
    "    mediaPrecisaoFalsas = round(sum(precisoesFalsas) / len(precisoesFalsas), 4)\n",
    "    mediasPrecisoesFalsasGJ.append(mediaPrecisaoFalsas)\n",
    "\n",
    "    mediaSensibilidadeReais = round(sum(sensibilidadeReais) / len(sensibilidadeReais), 4)\n",
    "    mediasSensibilidadeReaisGJ.append(mediaSensibilidadeReais)\n",
    "\n",
    "    mediaSensibilidadeFalsas = round(sum(sensibilidadeFalsas) / len(sensibilidadeFalsas), 4)\n",
    "    mediasSensibilidadeFalsasGJ.append(mediaSensibilidadeFalsas)\n",
    "\n",
    "    mediaF1Reais = round(sum(f1Reais) / len(f1Reais), 4)\n",
    "    mediasf1ScoreReaisGJ.append(mediaF1Reais)\n",
    "\n",
    "    mediaF1Falsas = round(sum(f1Falsas) / len(f1Falsas), 4)\n",
    "    mediasf1ScoreFalsasGJ.append(mediaF1Falsas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fF = [], fR = [], rF = [], rR = []\n",
    "for i in range(len(matrizesDeConfusao)):\n",
    "    fF.append(matrizesDeConfusao[i].loc['fF', 'num'])\n",
    "    fR.append(matrizesDeConfusao[i].loc['fR', 'num'])\n",
    "    rF.append(matrizesDeConfusao[i].loc['rF', 'num'])\n",
    "    rR.append(matrizesDeConfusao[i].loc['rR','num'])\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_medias = {\n",
    "    'Métrica': [\n",
    "        'Acuracidade GJ', \n",
    "        'Precisão Reais GJ', \n",
    "        'Precisão Falsas GJ', \n",
    "        'Sensibilidade Reais GJ', \n",
    "        'Sensibilidade Falsas GJ', \n",
    "        'F1-Score Reais GJ', \n",
    "        'F1-Score Falsas GJ',\n",
    "        'Acuracidade BC', \n",
    "        'Precisão Reais BC', \n",
    "        'Precisão Falsas BC', \n",
    "        'Sensibilidade Reais BC', \n",
    "        'Sensibilidade Falsas BC', \n",
    "        'F1-Score Reais BC', \n",
    "        'F1-Score Falsas BC'\n",
    "    ],\n",
    "    'Média': [\n",
    "        mediasAcuracidadesGJ,\n",
    "        mediasPrecisoesReaisGJ,\n",
    "        mediasPrecisoesFalsasGJ,\n",
    "        mediasSensibilidadeReaisGJ,\n",
    "        mediasSensibilidadeFalsasGJ,\n",
    "        mediasf1ScoreReaisGJ,\n",
    "        mediasf1ScoreFalsasGJ,\n",
    "        \n",
    "        mediasAcuracidadesBC,\n",
    "        mediasPrecisoesReaisBC,\n",
    "        mediasPrecisoesFalsasBC,\n",
    "        mediasSensibilidadeReaisBC,\n",
    "        mediasSensibilidadeFalsasBC,\n",
    "        mediasf1ScoreReaisBC,\n",
    "        mediasf1ScoreFalsasBC\n",
    "    ]\n",
    "}\n",
    "\n",
    "dfMedias = pd.DataFrame(dados_medias)\n",
    "\n",
    "print(dfMedias)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
